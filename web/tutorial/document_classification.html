<html><body>
<table border="0" cellpadding="0" cellspacing="0" width="980" bgcolor="#f4f0e5">
<tr><td background="../images/bar.png"><br>
</td></tr><tr><td>
<a href="../docs.html">Back to the docs page</a><br>

<br>
<a href="collaborative_filtering.html">Previous</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="start_coding.html">Next</a>







<h2>Document classification</h2>

<p>This page will visit some of the techniques used for document classification.</p>

<p>Let's suppose that you have gathered a collection of training documents in .txt or .html format. These documents are separated into three folders named "ham", "spam", and "auto_responses". You'd like to train some classification algorithm to decide in which folder new documents probably belong.</p>

<p>Since most classification algorithms are designed to operate on structured tables of information, the first step is to convert the collection of documents into a table format. This is typically done by assigning an attribute to each possible word, and a row to each document. Each element in the table will specify some sort of frequency-count for the number of times that a certain word occurs in a certain document.</p>

<p>As you might predict, this is going to be a humongous table! Fortunately, it is going to be very sparse. That is, most elements in the table are going to have a value of 0. If we re</p>

</p>The "waffles_generate" application contains a tool that will convert a bunch of documents to a sparse matrix in this manner:
<pre>
	waffles_generate docstosparsematrix ham spam auto_responses
</pre>
This creates a file named "docs.sparse", which is a sparse matrix representing all of the word counts in all of the documents in those three folders.</p>

</p>The values in each element of the matrix are computed as a*log(b/c)/d, where 'a' is the number of times the word occurs in this document, 'b' is the total number of documents, 'c' is the number of documents that contain this word, and 'd' is the max number of times this word occurs in any document.</p>

<p>If you are planning to use naive Bayes to classify your documents, a common choice, then you need discrete values, instead of real values, in your matrix. You can use the "-binary" flag to tell it to just use a 0 or a 1, depending on whether the word occurs at all in each document. Also, by default, the words are stemmed using the Porter stemming algorithm. If you're not using English, this will probably just make a mess of your words. You can suppress this feature with the "-nostem" flag. Also, you can specify the output filename. Here's an example:
<pre>
	waffles_generate docstosparsematrix -outfile mydocs.sparse -binary -nostem ham spam auto_responses
</pre></p>

<p>Now that you've got your data into a sparse matrix, you might want to shuffle the rows, and perhaps divide the data into a training and test set.
<pre>
	waffles_transform sparseshuffle mydocs.sparse > shuffled.sparse
	waffles_transform sparsesplit shuffled.sparse 250 train.sparse test.sparse
</pre></p>

<p>Now, it's time to train a classifier. First, you need to select a classification algorithm that can be trained with a sparse matrix. These are the algorithms that inherit from GIncrementalLearner. To see the full list of choices, go the the <a href="../apidoc/html/index.html">API docs</a>, open up the "class hierarchy", then navigate to GTransducer->GSupervisedLearner->GIncrementalLearner. The classes that inherit from GIncrementalLearner are the ones that can train on a sparse matrix. GKNN and GNaiveBayes are probably the most common choices for this task.</p>



<br>
<a href="collaborative_filtering.html">Previous</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="start_coding.html">Next</a>

<br><br><a href="../docs.html">Back to the docs page</a>
</td></tr><tr><td background="../images/bar.png"><br>
</td></tr></table>
</body></html>
